{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11581593,"sourceType":"datasetVersion","datasetId":7261790}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. SETUP AND IMPORTS","metadata":{"_uuid":"df9e79f7-8576-48e0-a277-751b65f9413c","_cell_guid":"6eedd759-eca7-4442-b5d3-89399bb6ddde","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport re\nimport pandas\nimport time\nfrom IPython.display import display\nimport string\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, LeakyReLU\nfrom tensorflow.keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu\n\nprint(f\"TensorFlow Version: {tf.__version__}\")","metadata":{"_uuid":"3834e32b-8e6c-42fb-9f05-7ce7f77e9c15","_cell_guid":"55f7b4d9-a100-4d8c-8a6c-f0ff55a8b25e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_DIR = \"/kaggle/input/flickr8k\"\nWORKING_DIR = \"/kaggle/working/\"\nSTARTSEQ = \"<start>\"\nENDSEQ = \"<end>\"\nCAPTIONS_FILE = os.path.join(BASE_DIR, 'captions.txt')\nIMAGES_DIR = os.path.join(BASE_DIR, 'Images')\nFEATURES_FILE = os.path.join(WORKING_DIR, \"features_vgg.pkl\")\nTOKENIZER_FILE =  os.path.join(WORKING_DIR, \"tokenizer.pkl\")\nMODEL_FILE = os.path.join(WORKING_DIR, \"best_model.keras\")","metadata":{"_uuid":"a1fcbf36-87ff-4eea-852b-bb5db11498c8","_cell_guid":"59d92d73-ce3c-43d1-b764-7e4bcee3a05e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. EXTRACT FEATURES","metadata":{"_uuid":"05d929bd-d3f9-4f7c-9365-24632233226c","_cell_guid":"405fbd67-9dbe-4970-a28f-2ac4b882e749","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"vgg = VGG16()\nfeatures_extractor = Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)\n#plot_model(vgg, show_shapes=True)","metadata":{"_uuid":"8e5fa845-c8ed-43d7-baa9-77a1c4020656","_cell_guid":"10bfff7f-feaa-4be1-ac74-a15804b1d7e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = {}\n\nif os.path.exists(FEATURES_FILE) :\n    print(\"Features already exists, loading features... \")\n    with open(FEATURES_FILE, 'rb') as file:\n        features = pickle.load(file)\nelse:\n    print(\"Extracting features...\")\n    for img_name in tqdm(os.listdir(IMAGES_DIR)):\n        img_path = os.path.join(IMAGES_DIR, img_name)\n        img = load_img(img_path, target_size=(224, 224))\n        img = img_to_array(img)\n        img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n        img = preprocess_input(img)\n        feature = features_extractor.predict(img, verbose=0)\n        img_id = img_name.split('.')[0]\n        features[img_id] = feature\n    \n    print(f\"Done. Extracted features from {len(features)} images \")\n    with open(FEATURES_FILE, 'wb') as file:\n        print(f\"Saving features to features_vgg.pkl\")\n        pickle.dump(features, file)","metadata":{"_uuid":"d739424a-f274-4258-8017-f6c7c6e337c0","_cell_guid":"75b97786-2048-451e-a799-cbb890e66d3b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. LOAD AND PROCESS CAPTIONS","metadata":{"_uuid":"d4ba34db-6db0-4929-8d33-1766f6ed9a17","_cell_guid":"04a3916f-6ae4-44ba-8f8b-0163b8003ef8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"df = pd.read_csv(CAPTIONS_FILE)\nprint(df.head())","metadata":{"_uuid":"38713b72-083f-4f60-b50c-8ea13eef41c8","_cell_guid":"49cb26dc-8827-453f-89fe-aa7ad7e3b2d7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef clean_caption(caption: str) -> str:\n    # Convert to lowercase\n    caption = caption.lower()\n    # Remove special characters (keep only letters and spaces)\n    caption = re.sub(r\"[^a-z\\s]\", \"\", caption)\n    # Replace multiple spaces with one space\n    caption = re.sub(r\"\\s+\", \" \", caption)\n    # Add <start> and <end> tokens\n    caption = f\"{STARTSEQ} {' '.join([word for word in caption.split() if len(word) > 1])} {ENDSEQ}\"\n    return caption\n\nmapping = {}\nfor _, row in tqdm(df.iterrows(), 'Processing and mapping captions'):\n    image_id = row[\"image\"].split('.')[0]\n    caption = row[\"caption\"]\n    \n    cleaned_caption = clean_caption(caption)\n    \n    if image_id not in mapping:\n        mapping[image_id] = []\n    mapping[image_id].append(cleaned_caption)\n\nprint(f\"Mapped captions for {len(mapping)} images \")","metadata":{"_uuid":"79f89c43-41a5-4667-8b38-6f0d3530a4be","_cell_guid":"64f00a2c-3a37-4b3f-a529-666df8e55bbe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Cpations of 1000268201_693b08cb0e.jpg\")\nfor c in mapping['1000268201_693b08cb0e']:\n    print(f\"- {c}\")","metadata":{"_uuid":"862b78c3-ee5f-4fd2-9748-12c738147949","_cell_guid":"86442cda-8fc2-47d1-a807-c89c9771d9b0","trusted":true,"collapsed":false,"execution":{"execution_failed":"2025-06-15T18:30:21.395Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_captions = [caption for captions in mapping.values() for caption in captions]\nprint(f\"Loaded {len(all_captions)} captions\")","metadata":{"_uuid":"99e60158-e609-4922-9fa4-ea1508dc93ee","_cell_guid":"a8845cdc-c89f-4399-a899-a18744659de9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. TOKENIZATION","metadata":{"_uuid":"5b6a526a-1a45-4426-ac3a-7df1d07e26b8","_cell_guid":"5d1526e9-942b-46b2-9a75-ba0dfcfd505f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1\nprint(f\"We have {vocab_size} vocabularies in captions\")","metadata":{"_uuid":"06fa412d-7596-425c-b278-8879af2ff30b","_cell_guid":"ef71b084-758e-4888-9117-f0e71555cb90","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_length = max([len(caption) for caption in all_captions])\nprint(f\"Max length in captions is {max_length}\")","metadata":{"_uuid":"8288a3ee-8e1d-413e-906b-091b4cb34081","_cell_guid":"5574bf8a-e4fb-48e3-bac5-bfaa1817661b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. TRAIN TEST SPLIT","metadata":{"_uuid":"0640e7d7-6c20-4ccf-9c3e-0430eda81ad5","_cell_guid":"2f9cb236-4c27-4fc2-a753-03a66fba4af3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"img_ids = list(mapping.keys())\nsplit_size = int(len(img_ids) * 0.95)\ntrain, test = img_ids[:split_size], img_ids[split_size:]\n\nlen(train), len(test)","metadata":{"_uuid":"ecb36774-d1f5-4b50-9b16-b26d805f4f9d","_cell_guid":"0a719356-6e45-43cb-a068-e5472091c4d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. DATA GENERATOR","metadata":{"_uuid":"60caddc9-6569-4c64-8f38-1f3b80098995","_cell_guid":"46d87203-f491-45e0-8626-936b1ae29250","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def data_generator(data_keys, mapping, features, max_length, vocab_size, batch_size):\n    # Initialize lists to hold batches of data\n    X1_batch, X2_batch, y_batch = [], [], []\n    n = 0  # Counter for the number of images processed in the current batch\n\n    # Infinite loop to generate data batches\n    while True:\n        # Iterate over the image IDs in the current epoch\n        for key in data_keys:\n            # Increment image counter\n            n += 1\n            # Get all captions for the current image\n            captions = mapping[key]\n\n            # Iterate over each caption for the current image\n            for caption in captions:\n                # Convert caption to a sequence of token IDs\n                seq = tokenizer.texts_to_sequences([caption])[0]\n\n                # Create input-output pairs from the sequence\n                # For a caption \"start word1 word2 end\", the pairs are:\n                # (start, word1), (start word1, word2), (start word1 word2, end)\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n\n                    # Pad the input sequence to the maximum length\n                    in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n\n                    # One-hot encode the output sequence (the next word)\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\n                    # Append the generated pair to the batch lists\n                    X1_batch.append(features[key][0])  # Image features for the current image\n                    X2_batch.append(in_seq)            # Padded input sequence\n                    y_batch.append(out_seq)            # One-hot encoded output word\n\n            # If the batch size is reached, yield the current batch\n            if n == batch_size:\n                # Convert lists to NumPy arrays\n                X1_batch_np = np.array(X1_batch, dtype=np.float32)\n                X2_batch_np = np.array(X2_batch, dtype=np.int32)\n                y_batch_np = np.array(y_batch, dtype=np.float32)\n\n                # Yield the batch as a tuple of (inputs, outputs)\n                # Inputs are a dictionary matching the model's input layer names\n                yield {\"image_features\": tf.constant(X1_batch_np),\n                       \"caption_input\": tf.constant(X2_batch_np)}, tf.constant(y_batch_np)\n\n                # Clear the batch lists for the next batch\n                X1_batch, X2_batch, y_batch = [], [], []\n                n = 0\n\n        # After iterating through all images, if there are remaining samples, yield a partial batch\n        if n > 0:\n            X1_batch_np = np.array(X1_batch, dtype=np.float32)\n            X2_batch_np = np.array(X2_batch, dtype=np.int32)\n            y_batch_np = np.array(y_batch, dtype=np.float32)\n\n            yield {\"image_features\": tf.constant(X1_batch_np),\n                   \"caption_input\": tf.constant(X2_batch_np)}, tf.constant(y_batch_np)\n\n            X1_batch, X2_batch, y_batch = [], [], []\n            n = 0","metadata":{"_uuid":"d1574aea-0d1a-426a-83f1-cbaf6cbc5943","_cell_guid":"0aab7268-e351-4048-9bff-80abffaf6205","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. MODEL ARCHITECTURE","metadata":{"_uuid":"8a1a4d92-e91c-44b0-be41-35089a7e235d","_cell_guid":"fd2f0d4d-35d6-4588-bd5f-cf4e86d052aa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"inputs1 = Input(shape=(4096,), name=\"image_features\")\nfe1 = Dropout(0.3)(inputs1)\nfe2 = Dense(256)(fe1)\nfe2 = LeakyReLU(negative_slope=0.1)(fe2)\n\n# Sequence model\ninputs2 = Input(shape=(max_length,), name=\"caption_input\")\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\nse2 = Dropout(0.3)(se1)\nse3 = LSTM(256)(se2)\n\n# Fusion des deux branches (image + texte)\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256)(decoder1)\ndecoder2 = LeakyReLU(negative_slope=0.1)(decoder2)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\n# Compilation et visualisation\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nplot_model(model,to_file='model.png',show_shapes=True,show_dtype=True,\n           show_layer_activations=False,show_trainable=True)","metadata":{"_uuid":"114e1ade-921d-4103-a354-d5c452c61e37","_cell_guid":"f73e449f-c671-45ce-b50e-db068bfc9624","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2025-06-15T18:30:21.396Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. TRAIN THE MODEL","metadata":{"_uuid":"37619d5a-6c14-45bd-affb-fcfd8b8a484c","_cell_guid":"096670f5-3773-4dfd-aea8-25b65be48f97","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"if not os.path.exists(MODEL_FILE):\n    epochs = 64\n    batch_size = 32\n    steps = len(train) // batch_size\n    \n    output_signature = (\n        {\n            \"image_features\": tf.TensorSpec(shape=(None, 4096), dtype=tf.float32),\n            \"caption_input\": tf.TensorSpec(shape=(None, max_length), dtype=tf.int32)\n        },\n        tf.TensorSpec(shape=(None, vocab_size), dtype=tf.float32)\n    )\n    \n    dataset = tf.data.Dataset.from_generator(\n        lambda: data_generator(\n            data_keys=train,\n            mapping=mapping,\n            features=features,\n            max_length=max_length,\n            vocab_size=vocab_size,\n            batch_size=batch_size\n        ),\n        output_signature=output_signature\n    )\n    \n    # Train the model using the dataset\n    history = model.fit(\n        dataset,\n        epochs=epochs,\n        steps_per_epoch=steps,\n        verbose=1\n    )\n    \n    model.save(MODEL_FILE)\n\n    plt.plot(history.history['loss'], label='Training loss')\n    \n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.show()","metadata":{"_uuid":"81614aaf-6485-4994-bc35-e90fa0a48f4b","_cell_guid":"faf790a1-85e9-497f-bac9-90c4bc47ff8b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. EVALUATE DU MODEL","metadata":{"_uuid":"81b66f30-1b25-469e-ba6d-485763d4f26d","_cell_guid":"43f73db6-2d35-4971-9754-1a37f3200c5b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"with open(FEATURES_FILE, 'rb') as file:\n    features = pickle.load(file)\nmodel = load_model(MODEL_FILE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n\n    return None\n\ndef predict_caption(model, image, tokenizer, max_length):    \n    in_text = STARTSEQ\n    for i in range(max_length):\n        seq = tokenizer.texts_to_sequences([in_text])[0]\n        seq = pad_sequences([seq], max_length, padding='post')\n        yhat = model.predict([image, seq], verbose=0)\n        yhat = np.argmax(yhat)\n        word = idx_to_word(yhat, tokenizer)\n        if word is None or word == ENDSEQ or word == \"end\":\n            break\n        in_text += \" \" + word\n    return in_text[8:] # remove \"<start> \"","metadata":{"_uuid":"97143bd0-e16c-40bb-b070-9f9b6c9de087","_cell_guid":"56c43a09-45ba-46d3-a55d-39bd8b58ff92","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"actual, predicted = [], []\n\nfor key in tqdm(test):\n    captions = mapping[key]\n    y_pred=predict_caption(model, features[key], tokenizer, max_length)\n    actual_captions = [caption.split() for caption in captions]\n    y_pred=y_pred.split()\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n\nprint(f\"BLEU-1: {corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))}\")\nprint(f\"BLEU-2: {corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))}\")","metadata":{"_uuid":"5ba180ff-3564-433a-b7e0-e45845e390ae","_cell_guid":"efb8a9fb-e3b5-4a7e-a986-0dcf29afeb5f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. TEST ET VISUALIZE RESULTS","metadata":{"_uuid":"3e56fb73-5988-444a-bfc1-46b3e7063063","_cell_guid":"c17dc0b4-da41-479a-a45f-5ae884af0b6a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import random\n\nimage_name = random.choice(test) + \".jpg\"\nimage_id = image_name.split('.')[0]\nimage_path = os.path.join(IMAGES_DIR, image_name)\nimage = Image.open(image_path)\ncaptions = mapping[image_id]\n\nprint(\"================ Actual Captions ===================\")\nfor caption in captions:\n    print(caption[8:-6])\n\ny_pred = predict_caption(model, features[image_id], tokenizer, max_length)\nprint(\"================ Predicted Captions ===================\")\n\nprint(y_pred)\nplt.imshow(image)","metadata":{"_uuid":"ed88efa4-d38f-4dde-8f78-8aef98e33b5c","_cell_guid":"24b4f3ba-9dfa-41a5-95bd-da5bfc4aa7fa","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}